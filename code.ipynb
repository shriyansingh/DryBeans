{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\singh\\AppData\\Local\\Temp\\ipykernel_4376\\2907253313.py:7: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data = data.replace('?', np.nan)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "data = pd.read_excel('DryBeanDataSet.xlsx')\n",
    "\n",
    "# Data Quality Issues Handling\n",
    "\n",
    "# 1. Missing Values\n",
    "data = data.replace('?', np.nan)\n",
    "\n",
    "# Handle missing values for numerical features\n",
    "numerical_features = ['Area', 'Perimeter', 'MajorAxisLength', 'MinorAxisLength', 'AspectRation', \n",
    "                      'Eccentricity', 'ConvexArea', 'EquivDiameter', 'Extent', 'Solidity', \n",
    "                      'roundness', 'Compactness', 'ShapeFactor1', 'ShapeFactor2', 'ShapeFactor3', \n",
    "                      'ShapeFactor4', 'ShapeFactor5', 'ShapeFactor6']\n",
    "\n",
    "data_knn = data.copy()\n",
    "for feature in numerical_features:\n",
    "    data_knn[feature] = data_knn[feature].fillna(data_knn[feature].median())\n",
    "\n",
    "data_dt = data.copy()\n",
    "\n",
    "data_knn = data_knn.dropna(subset=['Class'])\n",
    "data_dt = data_dt.dropna(subset=['Class'])\n",
    "\n",
    "# 2. Outliers\n",
    "def handle_outliers(df, columns):\n",
    "    for col in columns:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        df[col] = np.where(df[col] > upper_bound, upper_bound, df[col])\n",
    "        df[col] = np.where(df[col] < lower_bound, lower_bound, df[col])\n",
    "    return df\n",
    "\n",
    "data_knn = handle_outliers(data_knn, numerical_features)\n",
    "data_dt = handle_outliers(data_dt, numerical_features)\n",
    "\n",
    "# Remove negative values in ConvexArea\n",
    "data_knn = data_knn[data_knn['ConvexArea'] >= 0]\n",
    "data_dt = data_dt[data_dt['ConvexArea'] >= 0]\n",
    "\n",
    "for df in [data_knn, data_dt]:\n",
    "    df['Extent'] = pd.to_numeric(df['Extent'], errors='coerce')\n",
    "    df['Compactness'] = pd.to_numeric(df['Compactness'], errors='coerce')\n",
    "    df['ShapeFactor6'] = pd.to_numeric(df['ShapeFactor6'], errors='coerce')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_knn = data_knn.drop('Class', axis=1)\n",
    "y_knn = data_knn['Class']\n",
    "X_dt = data_dt.drop('Class', axis=1)\n",
    "y_dt = data_dt['Class']\n",
    "\n",
    "# splitter\n",
    "X_train_knn, X_test_knn, y_train_knn, y_test_knn = train_test_split(X_knn, y_knn, test_size=0.2, random_state=42, stratify=y_knn)\n",
    "X_train_dt, X_test_dt, y_train_dt, y_test_dt = train_test_split(X_dt, y_dt, test_size=0.2, random_state=42, stratify=y_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA plot saved as 'pca_cumulative_variance.png'\n",
      "Number of components explaining 95% variance: 8\n"
     ]
    }
   ],
   "source": [
    "numerical_columns = data_knn.select_dtypes(include=[np.number]).columns\n",
    "categorical_columns = data_knn.select_dtypes(exclude=[np.number]).columns\n",
    "\n",
    "# Encode categorical variables\n",
    "le = LabelEncoder()\n",
    "for col in categorical_columns:\n",
    "    data_knn[col] = le.fit_transform(data_knn[col].astype(str))\n",
    "\n",
    "def plot_pca_variance(X):\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    pca = PCA()\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    cumulative_variance_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(cumulative_variance_ratio) + 1), cumulative_variance_ratio, 'bo-')\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel('Cumulative Explained Variance Ratio')\n",
    "    plt.title('PCA Cumulative Explained Variance Ratio')\n",
    "    plt.grid(True)\n",
    "    plt.savefig('pca_cumulative_variance.png')\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"PCA plot saved as 'pca_cumulative_variance.png'\")\n",
    "    \n",
    "    # Return the number of components that explain 95% of the variance\n",
    "    return np.argmax(cumulative_variance_ratio >= 0.95) + 1\n",
    "\n",
    "# Apply PCA to numerical features only\n",
    "X_knn_numerical = data_knn[numerical_columns]\n",
    "n_components = plot_pca_variance(X_knn_numerical)\n",
    "print(f\"Number of components explaining 95% variance: {n_components}\")\n",
    "\n",
    "# Now, create the final feature set for k-NN\n",
    "X_knn_pca = pd.DataFrame(\n",
    "    PCA(n_components=n_components).fit_transform(StandardScaler().fit_transform(X_knn_numerical)),\n",
    "    columns=[f'PC{i+1}' for i in range(n_components)]\n",
    ")\n",
    "\n",
    "# Add back the encoded categorical features\n",
    "for col in categorical_columns:\n",
    "    X_knn_pca[col] = data_knn[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "numerical_columns = X_train_knn.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_columns = X_train_knn.select_dtypes(include=['object']).columns\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ]), numerical_columns),\n",
    "        ('cat', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), categorical_columns)\n",
    "    ])\n",
    "\n",
    "# k-NN specific pipeline\n",
    "knn_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('pca', PCA(n_components=0.95)),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "# Decision Tree specific pipeline\n",
    "dt_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('dt', DecisionTreeClassifier())\n",
    "])\n",
    "\n",
    "knn_param_grid = {\n",
    "    'knn__n_neighbors': [3, 5, 7, 9, 11],\n",
    "    'knn__weights': ['uniform', 'distance'],\n",
    "    'knn__metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "\n",
    "dt_param_grid = {\n",
    "    'dt__max_depth': [5, 10, 15, 20, None],\n",
    "    'dt__min_samples_split': [2, 5, 10],\n",
    "    'dt__min_samples_leaf': [1, 2, 4],\n",
    "    'dt__criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "knn_grid_search = GridSearchCV(knn_pipeline, knn_param_grid, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "knn_grid_search.fit(X_train_knn, y_train_knn)\n",
    "\n",
    "dt_grid_search = GridSearchCV(dt_pipeline, dt_param_grid, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "dt_grid_search.fit(X_train_dt, y_train_dt)\n",
    "\n",
    "#best modlle\n",
    "best_knn = knn_grid_search.best_estimator_\n",
    "best_dt = dt_grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "K-Nearest Neighbors Results:\n",
      "Best Parameters: {'knn__metric': 'euclidean', 'knn__n_neighbors': 11, 'knn__weights': 'uniform'}\n",
      "Best Cross-Validation Score: 0.9241307756717667\n",
      "\n",
      "Cross-validation results:\n",
      "Mean Accuracy: 0.9198 (+/- 0.0053)\n",
      "Best Accuracy: 0.9241\n",
      "Worst Accuracy: 0.9109\n",
      "\n",
      "Decision Tree Results:\n",
      "Best Parameters: {'dt__criterion': 'gini', 'dt__max_depth': 20, 'dt__min_samples_leaf': 4, 'dt__min_samples_split': 10}\n",
      "Best Cross-Validation Score: 0.9791243307705969\n",
      "\n",
      "Cross-validation results:\n",
      "Mean Accuracy: 0.9683 (+/- 0.0037)\n",
      "Best Accuracy: 0.9791\n",
      "Worst Accuracy: 0.8997\n"
     ]
    }
   ],
   "source": [
    "def print_results(grid_search, model_name):\n",
    "    print(f\"\\n{model_name} Results:\")\n",
    "    print(\"Best Parameters:\", grid_search.best_params_)\n",
    "    print(\"Best Cross-Validation Score:\", grid_search.best_score_)\n",
    "    \n",
    "    means = grid_search.cv_results_['mean_test_score']\n",
    "    stds = grid_search.cv_results_['std_test_score']\n",
    "    \n",
    "    print(f\"\\nCross-validation results:\")\n",
    "    print(f\"Mean Accuracy: {means.mean():.4f} (+/- {stds.mean()*2:.4f})\")\n",
    "    print(f\"Best Accuracy: {means.max():.4f}\")\n",
    "    print(f\"Worst Accuracy: {means.min():.4f}\")\n",
    "\n",
    "print_results(knn_grid_search, \"K-Nearest Neighbors\")\n",
    "print_results(dt_grid_search, \"Decision Tree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-NN Performance:\n",
      "Accuracy: 0.9187\n",
      "Precision: 0.9204\n",
      "Recall: 0.9187\n",
      "F1-score: 0.9189\n",
      "\n",
      "Confusion Matrix:\n",
      "[[225   0  29   0   1   1   7]\n",
      " [  0 104   0   0   0   0   0]\n",
      " [  4   4 308   0   5   1   4]\n",
      " [  0   0   0 655   1   9  44]\n",
      " [  0   0   8   3 365   0   9]\n",
      " [  2   0   0   8   0 377  18]\n",
      " [  0   0   2  51   6   4 464]]\n",
      "Confusion matrix plot saved as 'k-nn_confusion_matrix.png'\n",
      "\n",
      "\n",
      "Decision Tree Performance:\n",
      "Accuracy: 0.9812\n",
      "Precision: 0.9812\n",
      "Recall: 0.9812\n",
      "F1-score: 0.9812\n",
      "\n",
      "Confusion Matrix:\n",
      "[[240   1  18   0   0   0   4]\n",
      " [  0 104   0   0   0   0   0]\n",
      " [ 15   0 307   0   0   0   4]\n",
      " [  0   0   0 708   0   0   1]\n",
      " [  0   0   0   0 385   0   0]\n",
      " [  0   0   0   0   0 404   1]\n",
      " [  7   0   0   0   0   0 520]]\n",
      "Confusion matrix plot saved as 'decision_tree_confusion_matrix.png'\n",
      "\n",
      "\n",
      "Best k-NN Parameters:\n",
      "{'knn__metric': 'euclidean', 'knn__n_neighbors': 11, 'knn__weights': 'uniform'}\n",
      "\n",
      "Best Decision Tree Parameters:\n",
      "{'dt__criterion': 'gini', 'dt__max_depth': 20, 'dt__min_samples_leaf': 4, 'dt__min_samples_split': 10}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "knn_predictions = best_knn.predict(X_test_knn)\n",
    "dt_predictions = best_dt.predict(X_test_dt)\n",
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    \n",
    "    print(f\"{model_name} Performance:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-score: {f1:.4f}\")\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(cm)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'Confusion Matrix - {model_name}')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.savefig(f'{model_name.lower().replace(\" \", \"_\")}_confusion_matrix.png')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Confusion matrix plot saved as '{model_name.lower().replace(' ', '_')}_confusion_matrix.png'\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "evaluate_model(y_test_knn, knn_predictions, \"k-NN\")\n",
    "evaluate_model(y_test_dt, dt_predictions, \"Decision Tree\")\n",
    "\n",
    "# Print best parameters\n",
    "print(\"Best k-NN Parameters:\")\n",
    "print(knn_grid_search.best_params_)\n",
    "print(\"\\nBest Decision Tree Parameters:\")\n",
    "print(dt_grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "k-NN Cross-validation Results:\n",
      "Mean Accuracy: 0.9241\n",
      "Standard Deviation: 0.0014\n",
      "\n",
      "Decision Tree Cross-validation Results:\n",
      "Mean Accuracy: 0.9791\n",
      "Standard Deviation: 0.0015\n",
      "Learning curve saved as 'k-nn_learning_curve_learning_curve.png'\n",
      "Learning curve saved as 'decision_tree_learning_curve_learning_curve.png'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\nk-NN Cross-validation Results:\")\n",
    "print(f\"Mean Accuracy: {knn_grid_search.best_score_:.4f}\")\n",
    "print(f\"Standard Deviation: {knn_grid_search.cv_results_['std_test_score'][knn_grid_search.best_index_]:.4f}\")\n",
    "\n",
    "print(\"\\nDecision Tree Cross-validation Results:\")\n",
    "print(f\"Mean Accuracy: {dt_grid_search.best_score_:.4f}\")\n",
    "print(f\"Standard Deviation: {dt_grid_search.cv_results_['std_test_score'][dt_grid_search.best_index_]:.4f}\")\n",
    "\n",
    "# Learning curves\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "def plot_learning_curve(estimator, X, y, title):\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=5, n_jobs=-1, \n",
    "        train_sizes=np.linspace(0.1, 1.0, 5), scoring='accuracy')\n",
    "    \n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.grid()\n",
    "    \n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "    \n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.savefig(f'{title.lower().replace(\" \", \"_\")}_learning_curve.png')\n",
    "    plt.close()\n",
    "    print(f\"Learning curve saved as '{title.lower().replace(' ', '_')}_learning_curve.png'\")\n",
    "\n",
    "plot_learning_curve(best_knn, X_knn, y_knn, \"k-NN Learning Curve\")\n",
    "plot_learning_curve(best_dt, X_dt, y_dt, \"Decision Tree Learning Curve\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
